<TITLE>Information Is Not Uncertainty</TITLE>
<body bgcolor="#ffffff">
<DOCUMENT>
<center>
<H1>
Information Is Not Entropy,<br>
Information Is Not Uncertainty!
</H1>
</center>

  Dr. Thomas D. Schneider <br>
  National Institutes of Health <br>
  National Cancer Institute <br>
  Center for Cancer Research Nanobiology Program <br>
  Molecular Information Theory Group <br>
  Frederick, Maryland  21702-1201 <br>
  toms@ncifcrf.gov <br>

<a href =
"http://www.ccrnp.ncifcrf.gov/~toms/">http://www.ccrnp.ncifcrf.gov/~toms/</a>

<p>
<IMG SRC="icons/colorbar.gif">
</p>

<p>
There are many many statements in the literature which
say that information is the same as entropy.
The reason for this was told by
Tribus.
The story goes
that Shannon didn't know what to call his measure so he asked von Neumann,
who said `You should call it entropy ... [since] ...
no one knows what entropy really is, so in a debate you will always
have the advantage'
(<a href = "#tribus">Tribus1971</a>).
</p>

<p>
Shannon called his measure not only the entropy but
also the "uncertainty".
I prefer this term because it does not have physical
units associated with it.
If you correlate
information with uncertainty, then you get
into deep trouble.  Suppose that:
</p>
 
<p><center>
information ~ uncertainty <br>
</center>
 
<p>
but
since they have almost identical formulae: <br>
</p>
 
<center>
uncertainty ~ physical entropy <br>
</center>

so <br>

<p><center>
information ~ physical entropy <br>
</center>
</p>

BUT as a system gets more random, its entropy goes up: <br>

<p><center>
randomness ~ physical entropy <br>
</center>
</p>

so <br>

<p><center>
information ~ physical randomness <br>
</center>
</p>

<p>
How could that be?
Information is the very
opposite of randomness!
</p>

<p>
The confusion comes from neglecting to do a subtraction: <br>
</p>

<p>
<center>
<b>Information is always a measure of the decrease of uncertainty
at a receiver (or molecular machine).</b>
</center>
</p>

<p>
If you use this definition, it will clarify all
the confusion in the literature.
</p>

<p>
Note: Shannon understood this 
<!--
2004 Dec 15
Bogdan S. PECICAN pointed out that this is confusing:
distinction and called the subtracted uncertainty the 'equivocation'.
-->
distinction and called the uncertainty which is subtracted the 'equivocation'.
Shannon (<a href =
"http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html"
>1948</a>) said on page 20:
<br>
<center>
R = H(x) - Hy(x)
</center>
<br>
"The conditional entropy Hy(x)
will, for convenience, be called the equivocation.
It measures the average ambiguity of the received signal."
</p>

<p>
The mistake is almost always made by people who are not actually trying
to use the measure.
As a practical example, consider the
<a href = "sequencelogo.html">sequence logos</a>.
Further discussion on this topic is in
the
<a href="http://www.ccrnp.ncifcrf.gov/~toms/bionet.info-theory.faq.html">
http://www.ccrnp.ncifcrf.gov/~toms/bionet.info-theory.faq.html</a>
under the topic
<a href="http://www.ccrnp.ncifcrf.gov/~toms/bionet.info-theory.faq.html#Information.Equal.Entropy">
<i>I'm Confused: How Could Information Equal Entropy?</i></a>
</p>

<p>
For a more mathematical approach, see the
<a href = "http://www.ccrnp.ncifcrf.gov/~toms/paper/primer">
Information Theory Primer</a>.
</p>

<p>
Some
<a href = "http://www.ccrnp.ncifcrf.gov/~toms/conversation1.html">
questions and answers</a> might make these isues more clear.
</p>

<hr>

<strong>References</strong>

<ul>

<a name = tribus>
<li>
<p>
<pre>
@article{Tribus1971,
author = "M. Tribus
 and E. C. McIrvine",
title = "Energy and Information",
journal = "Sci. Am.",
volume = "225",
note = "{(Note: the table of contents in this volume
incorrectly lists this as volume {\bf 224})}",
number = "3",
pages = "179-188",
month = "September",
year = "1971"}
</pre>
</p>

</ul>


<strong>Examples of the error</strong>
<ul>

<li>
<p>
<pre>
@article{Machta1999,
author = "J. Machta",
title = "{Entropy, Information, and Computation}", 
journal = "Am. J. Phys.",
volume = "67",
pages = "1074-1077",
year = "1999"}   
</pre>
"The results of random processes usually have high information content".
"Randomness and information are formally the same thing."
He also shows an equation relating "Shannon information"
to the uncertainty function.
This is a perfect example of total confusion on this issue!
</p>

<li>
<p>
<pre>
@article{Padian2002,
author = "K. Padian",
title = "{EVOLUTION AND CREATIONISM:
Waiting for the Watchmaker}",
<a href =
"http://www.sciencemag.org/cgi/content/full/295/5564/2373?maxtoshow=&HITS=10&hits=10&RESULTFORMAT=&searchid=1018312006742_538&stored_search=&FIRSTINDEX=0&volume=295&firstpage=2373&fdate=10/1/1995&tdate=4/30/2002"
>journal = "Science",
volume = "295",   
pages = "2373-2374",
year = "2002"}</a>
</pre>
"In information theory, the term can imply increasing predictability or
increasing entropy, depending on the context."

Kevin Padian, who wrote the review, reports that the error
came from the book he was reviewing:
<pre>
 Intelligent Design Creationism and Its Critics
 Philosophical, Theological, and Scientific Perspectives
 Robert T. Pennock, Ed.
 MIT Press, Cambridge, MA, 2001. 825 pp. $110,
 £75.95. ISBN 0-262-16204-0. Paper, $45, £30.95.
 ISBN 0-262-66124-1.
</pre>
</p>

<li>
<p>
<pre>
@article{Allahverdyan.Nieuwenhuizen2001,
author = "A. E. Allahverdyan
 and T. H. Nieuwenhuizen",
title = "{Breakdown of the Landauer bound for information erasure in the
quantum regime}",
journal = "Phys. Rev. E", 
volume = "64",
pages = "056117-1--056117-9",
year = "2001"}
</pre>
This is an example of the <i>typical</i> physicists' muddle
about "erasure" in which they set the state of a device to one
of several states and call this a "loss of information".
But setting a device to one state (no matter what it is) 
decreases the entropy and increases the information.
The main mistake that the physicists make is not having
any real working examples.  It's entirely theoretical for them.
(These people believe that they can beat the Second Law.
I would simply ask them to build the perpetual motion machine
and run the world power grid from it before making such a claim.)
</p>

<a name = "Crow2001"></a>
<li>
<p>
<pre>
@article{Crow2001,
author = "J. F. Crow",
title = "{Shannon's brief foray into genetics}",
journal = "Genetics",
volume = "159",
pages = "915--917",
year = "2001"}
</pre>
He confounds information with uncertainty, but forgot the minus
sign on the sum p log p formula.
He also confounded information with entropy.
Finally, he claimed that "a noisy system can send an undistorted
signal provided that the appropriate error corrections or redundancy
are built in".  This is incorrect since there will always be error,
but Shannon's channel capacity theorem shows that the error
can be made as low as desired (but not zero as this author claims).
</p>

<a name = "Brissaud2005"></a>
<li>
<p>
<!--
<pre>
</pre>
-->
"Entropy measures lack of information;
it also measures information. These two conceptions are complementary. "
<a href =
"http://www.mdpi.org/entropy/htm/e7010068.htm"
>The meanings of entropy</a>,
Jean-Bernard Brissaud,
Entropy 2005, 7[1], 68-96.
</p>

<a name = "Weaver"></a>
<li>
<p>
2006 Oct 19:
<!-- mvanstaveren@epo.org -->
Martin Van Staveren pointed out that
<blockquote>
at the top of page 22
<a href =
"http://www.ccrnp.ncifcrf.gov/~toms/glossary.html#Shannon1948"
>Shannon's 1948 paper</a>,
it seems to be suggested,
that part of the received information is due to noise.  This is
obviously a slip of the pen of Shannon, as he merely tries to explain,
in words,  that the information rate R is the initial uncertainty
minus the uncertainty due to the noise; but he calls H "information"
instead of "entropy".
</blockquote>
He further pointed out that much of the confusion may have come
from Weaver:
<blockquote>
see this:
<a href =
"http://www.uoregon.edu/~felsing/virtual_asia/info.html"
>http://www.uoregon.edu/~felsing/virtual_asia/info.html</a>
<br>
This is part of the intro that Weaver wrote for "The mathematical theory 
of information".  Some people even refer to "Shannon-Weaver theory" 
because of this intro.
<br>
section 2.5 of this intro: noise generates "spurious", or "undesirable" 
information, whatever that may mean. The section also introduces the 
esoteric notion of "meaningless information", contrary to what Shannon 
himself says in the body of the text. I think that Weaver's arrogance in 
thinking that he had to "explain" Shannon, has done a big disservice to 
information theory, which really is only probability theory.
</blockquote>
<!--
<pre>
</pre>
-->
</p>

<a name = "Roche"></a>
<li>
<p>
<!--
While reading "God: The Failed Hypothesis. How Science Shows That God
Does Not Exist" by Victor J. Stenger I found this reference.
---
He is gone:
The following message to <droche@unsw.edu.au> was undeliverable.
The reason for the problem:
5.1.0 - Unknown address error 553-'5.1.1 <droche@unsw.edu.au>...
No such user droche@unsw.edu.au here'
-->
"A Bit Confused. Creationism and Information Theory" Skeptical
Inquirer, 3/1/01 David Roche.
<a href =
"http://findarticles.com/p/articles/mi_m2843/is_2_25/ai_71563254"
>(link 1)</a>
<a href =
"http://www.encyclopedia.com/doc/1G1-71563254.html"
>(link 2)</a>.
He made a mistake in his description of Shannon information.
What he described is the Shannon uncertainty.  Shannon information is
a difference between uncertainties.
Because of his confusion, he associates disorder with information.
</p>

<a name = "MITOCW"></a>
<li>
<p>
2009 Jan 21:
<a href =
"http://e2ma.net/go/1659920391/1506873/55292847/goto:http://ocw.mit.edu/OcwWeb/Electrical-Engineering-and-Computer-Science/6-050JSpring-2008/CourseHome/index.htm"
>6.050J Information and Entropy</a>
(Spring 2008)
an MIT Open Courseware course.  The preface reads:
"Only recently has entropy been widely accepted as a form of information."
which is, of course backwards.
</p>
<p>
Also, the statement
"Second Law states that entropy never decreases as time goes on"
is wrong since the entropy of a system can decreas if heat leaves
the system - that's how snowflakes form!
</p>
<!--
<p>
Another mistake:
"Also, information is inherently subjective, because it deals with
what you know and what you don't know (entropy, as one form of
information, is also subjective---this point makes some physicists
uneasy)."  This is clarified in chapter 1 page 2.
</p>
-->
<p>
At least they admit:
"In fact, we may not yet have it right."!!
</p>

</ul>

<hr>

See also
<a href =
"http://www.ccrnp.ncifcrf.gov/~toms/pitfalls.html"
>Pitfalls in Information Theory
and
Molecular Information Theory</a>

<p>
<IMG SRC="http://www.ccrnp.ncifcrf.gov/~toms/icons/colorbar.gif" width = 100% height =11>

<A HREF="http://www.ccrnp.ncifcrf.gov/~toms/"><IMG border=0 SRC =
"http://www.ccrnp.ncifcrf.gov/~toms/icons/tinyround.gif" align =
"left"><br><br><br>Schneider Lab</A><br>
origin: 1997 January 4 <br>
updated:
2009 Jan 21: MIT OCW
<!--
2009 Jan 21: MIT OCW
2007 Jan 14: Roche
2006 Oct 19: Weaver
2005 Aug 25: Brissaud
2005 Mar 23: rediscovered ... Machta1999
2004 Dec 17
2002 Jun 17
-->
<br clear = all>
</p>

<IMG SRC="http://www.ccrnp.ncifcrf.gov/~toms/icons/colorbar.gif" width = 100% height =11>

</DOCUMENT>
